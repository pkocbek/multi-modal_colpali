{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e62bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/primozk/miniconda3/envs/mm_colpali/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# colpali modules\n",
    "#poetry add git+https://github.com/illuin-tech/colpali.git\n",
    "#from colpali_engine.models import ColIdefics3, ColIdefics3Processor\n",
    "#from colpali_engine.models import ColQwen2_5Omni, ColQwen2_5OmniProcessor\n",
    "#from colpali_engine.models import ColQwen2_5, ColQwen2_5_Processor\n",
    "\n",
    "#poetry add git+https://github.com/AhmedMasryKU/colflor.git\n",
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "from colpali_engine.models import ColFlor, ColFlorProcessor\n",
    "\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from transformers.utils.import_utils import is_flash_attn_2_available\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\")\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "#import qdrant_client\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import pickle\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "\n",
    "\n",
    "#%pip install python-dotenv\n",
    "#openai api key\n",
    "import os\n",
    "%load_ext dotenv\n",
    "#%reload_ext dotenv\n",
    "%dotenv\n",
    "from openai._client import OpenAI\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8074540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = 'papers_merge/'\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"papers_merge\", exist_ok=True)\n",
    "os.makedirs(\"results/evals\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c3e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_images(pdf_dir):\n",
    "    \"\"\"\n",
    "    Converts all PDFs in a directory to images.\n",
    "\n",
    "    Args:\n",
    "        pdf_dir (str): Path to the directory containing PDFs.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are file names (without extension), \n",
    "              and values are lists of images (one list per PDF).\n",
    "    \"\"\"\n",
    "    pdf_list = [pdf for pdf in sorted(os.listdir(pdf_dir)) if pdf.endswith(\".pdf\")]\n",
    "    all_images = {}\n",
    "\n",
    "    for pdf_file in pdf_list:\n",
    "        pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "        pdf_images = convert_from_path(pdf_path)\n",
    "        all_images[pdf_file] = pdf_images  # Use file name as key\n",
    "    \n",
    "    return all_images\n",
    "\n",
    "#pdf_dir = 'papers_merge/'\n",
    "#all_images = convert_pdf_to_images(pdf_dir)\n",
    "#with open('data/pdf_images.pkl', 'wb') as file:\n",
    "#    pickle.dump(all_images, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba0161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_embeddings(pdf_dir, model, processor, batch_size=2):\n",
    "    \"\"\"\n",
    "    Converts all PDFs in a directory into embeddings with metadata.\n",
    "\n",
    "    Args:\n",
    "        pdf_dir (str): Directory containing PDF files.\n",
    "        model: Pre-trained model for generating embeddings.\n",
    "        processor: Preprocessor for the model (e.g., to process images).\n",
    "        batch_size (int): Batch size for inference.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains:\n",
    "            - \"embedding\": The embedding tensor.\n",
    "            - \"doc_id\": The document ID (int).\n",
    "            - \"page_id\": The page index within the document.\n",
    "            - \"file_name\": The name of the source PDF file.\n",
    "    \"\"\"\n",
    "    all_images = convert_pdf_to_images(pdf_dir)\n",
    "    all_embeddings_with_metadata = []\n",
    "\n",
    "    for doc_id, (file_name, pdf_images) in enumerate(all_images.items()):\n",
    "        dataloader = DataLoader(\n",
    "            dataset=pdf_images,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=lambda x: processor.process_images(x),\n",
    "        )\n",
    "\n",
    "        page_counter = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Processing {file_name}\"):\n",
    "            with torch.no_grad():\n",
    "                batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "                batch_embeddings = model(**batch)\n",
    "                batch_embeddings = list(torch.unbind(batch_embeddings.to(\"cpu\")))\n",
    "\n",
    "                for embedding in batch_embeddings:\n",
    "                    all_embeddings_with_metadata.append({\n",
    "                        \"embedding\": embedding,\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"page_id\": page_counter,\n",
    "                        \"file_name\": file_name,  # Correctly use the file name\n",
    "                    })\n",
    "                    page_counter += 1\n",
    "\n",
    "    return all_embeddings_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8454d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ds = create_document_embeddings(pdf_dir, model, processor, batch_size=4)\n",
    "\n",
    "#with open('data/colpali_pdf_emb.pkl', 'wb') as file:\n",
    "#    pickle.dump(ds, file)\n",
    "# with open('data/colpali_pdf_emb.pkl', 'rb') as fp:\n",
    "#     ds = pickle.load(fp)\n",
    "\n",
    "# print(f\"Generated embeddings for {len(ds)} pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d772f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(query, processor, model, ds, all_images, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieves top-k relevant images for a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query as a string.\n",
    "        processor: Processor for pre-processing the query.\n",
    "        model: Model to generate embeddings for the query.\n",
    "        ds (list): List of dictionaries with \"embedding\", \"doc_id\", \"page_id\", and \"file_name\".\n",
    "        all_images (dict): Dictionary of images per document.\n",
    "        top_k (int): Number of top results to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains:\n",
    "            - \"doc_id\": Document ID.\n",
    "            - \"page_id\": Page ID.\n",
    "            - \"file_name\": Name of the source PDF file.\n",
    "            - \"image\": The retrieved image (PIL.Image.Image).\n",
    "            - \"score\": Similarity score for the image.\n",
    "    \"\"\"\n",
    "    # Process the query and move to model's device\n",
    "    query_list = query if isinstance(query, list) else [query]\n",
    "    batch_queries = processor.process_queries(query_list).to(model.device)\n",
    "\n",
    "    # Forward pass to get query embeddings\n",
    "    with torch.no_grad():\n",
    "        query_embeddings = model(**batch_queries)\n",
    "\n",
    "    # Extract embeddings from ds for scoring\n",
    "    document_embeddings = torch.stack([entry[\"embedding\"] for entry in ds])\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    scores = processor.score_multi_vector(query_embeddings, document_embeddings)\n",
    "    #print(len(scores))\n",
    "\n",
    "    all_results=[]\n",
    "    for el in scores:\n",
    "\n",
    "        score_values = el.tolist()  # Extract similarity scores as a list\n",
    "\n",
    "        # Get top-k indices of the most relevant embeddings\n",
    "        top_indices = el.topk(top_k).indices.tolist()\n",
    "\n",
    "       # Retrieve corresponding images and metadata\n",
    "        retrieved_results = []\n",
    "        for idx in top_indices:\n",
    "            entry = ds[idx]\n",
    "            doc_id = entry[\"doc_id\"]\n",
    "            page_id = entry[\"page_id\"]\n",
    "            file_name = entry[\"file_name\"]\n",
    "            image = all_images[file_name][page_id]  # Correct lookup using file_name\n",
    "\n",
    "            # Add score to each result\n",
    "            retrieved_results.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"page_id\": page_id,\n",
    "                \"file_name\": file_name,\n",
    "                \"image\": image,\n",
    "                \"score\": score_values[idx],  # Add similarity score\n",
    "            })\n",
    "        all_results.append(retrieved_results)\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30908611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top_two_results(query, retrieved_results, nr_res=5):\n",
    "    \"\"\"\n",
    "    Displays the top two retrieved images in a 1x2 canvas using Matplotlib.\n",
    "\n",
    "    Args:\n",
    "        retrieved_results (list): A list of dictionaries with metadata and images.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    for el_q, el_ret in zip(query,retrieved_results):\n",
    "    # Take only the top two results\n",
    "        top_two_results = el_ret[:nr_res]\n",
    "\n",
    "        # Create a 1x2 canvas\n",
    "        fig, axes = plt.subplots(nr_res, 1, figsize=(16, 16))\n",
    "        print(f\"Query: {el_q}\")\n",
    "        for i, result in enumerate(top_two_results):\n",
    "            file_name = result[\"file_name\"]\n",
    "            page_id = result[\"page_id\"]\n",
    "            score = result[\"score\"]\n",
    "            image = result[\"image\"]\n",
    "\n",
    "            axes[i].imshow(image)\n",
    "            axes[i].axis('off')\n",
    "            axes[i].set_title(f\"File: {file_name}\\nPage: {page_id+1}\\nScore: {score:.4f}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "#display_top_two_results(query, retrieved_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_loc =\"./data/Glycans_q_a_v5.xlsx\"\n",
    "qa_data = pd.read_excel(qa_loc )\n",
    "\n",
    "#randomize order, de-randomize by question number after we get results + merge with other relevant columns\n",
    "qa_data = qa_data.sample(frac=1).reset_index(drop=True)\n",
    "#qa_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3572daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "class MCQ(BaseModel):\n",
    "   answer: Literal[\"A\", \"B\", \"C\", \"D\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a42cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from openai import OpenAI, RateLimitError,AsyncOpenAI\n",
    "import asyncio\n",
    "import backoff \n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, fixed_width=1024):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(BytesIO(img_data))\n",
    "    \n",
    "    width = fixed_width\n",
    "    img_ratio = img.size[0] / img.size[1]\n",
    "    height = int(width/img_ratio)\n",
    "    size_new = width, height\n",
    "\n",
    "    # Resize the image\n",
    "    #resized_img = img.resize(size, Image.LANCZOS)\n",
    "    resized_img =img.resize(size_new, resample=Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def img_context (context_imgs):\n",
    "    messages = []\n",
    "    for image in context_imgs:\n",
    "        # Save the resized image to a bytes buffer\n",
    "\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        img_str = base64.b64encode(buffered.getvalue())\n",
    "\n",
    "        resized_image = resize_base64_image(img_str)\n",
    "        image_message = {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{resized_image}\"},\n",
    "        }\n",
    "        messages.append(image_message)\n",
    "    \n",
    "    out_prompt =[{\"type\": \"text\",\"text\": \"Context information:\" }]\n",
    "    \n",
    "\n",
    "    return out_prompt + messages\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "# Decorator for exponential backoff with a maximum of 5 retries\n",
    "@backoff.on_exception(backoff.expo, RateLimitError, max_tries=5)\n",
    "async def get_completion_with_backoff(client, gpt_model, prompt_el, MCQ):\n",
    "    \"\"\"\n",
    "    Asynchronously sends a request to the OpenAI API with exponential backoff for rate limiting.\n",
    "\n",
    "    Args:\n",
    "        client (AsyncOpenAI): The asynchronous OpenAI client.\n",
    "        gpt_model (str): The model to use for the completion.\n",
    "        prompt_el (list): The prompt messages for the model.\n",
    "        MCQ (Pydantic.BaseModel): The response format for the completion.\n",
    "\n",
    "    Returns:\n",
    "        dict: The parsed JSON response from the API.\n",
    "    \"\"\"\n",
    "    completion = await client.beta.chat.completions.parse(\n",
    "        model=gpt_model,\n",
    "        messages=prompt_el,\n",
    "        response_format=MCQ\n",
    "    )\n",
    "    return json.loads(completion.choices[0].message.content)\n",
    "\n",
    "async def send_to_model_async(gpt_model, table_qa, no_context=1, topk=5, chunk=10, processor=[], model=[], ds=[], all_images=[]):\n",
    "    \"\"\"\n",
    "    Asynchronously sends questions to a model and retrieves answers, with support for context retrieval.\n",
    "\n",
    "    Args:\n",
    "        gpt_model (str): The GPT model to use for answering questions.\n",
    "        table_qa (pd.DataFrame): DataFrame with questions and answer options.\n",
    "        no_context (int): If 0, retrieves context; otherwise, sends questions directly.\n",
    "        topk (int): The number of top results to retrieve for context.\n",
    "        chunk (int): The chunk size for processing questions.\n",
    "        processor: The processor for the retrieval model.\n",
    "        model: The retrieval model.\n",
    "        ds: The dataset for retrieval.\n",
    "        all_images: A dictionary of all images.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing the output array of answers and a list of retrieved information.\n",
    "    \"\"\"\n",
    "    client = AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    \n",
    "    prompt_list = []\n",
    "    prompt_q = []\n",
    "    for _, row in table_qa.iterrows():\n",
    "        question = row['question']\n",
    "        resp = ['A', 'B', 'C', 'D']\n",
    "        question_string = \"\".join([f\"{letter}. {option}\" for letter, option in zip(resp, [row['A'], row['B'], row['C'], row['D']])])\n",
    "        \n",
    "        prompt = f\"\"\"\\\n",
    "        You are an experienced senior researcher tasked with providing in-depth analysis.\n",
    "        Use all the information at your disposal, such as uploaded files and other sources. Think about the following statement or question: {question}\\n\n",
    "        Below are the possible answers, where letters mark each answer. First, exclude the unlikely answer or answers, rethink, and select an output from the rest. The output is only ONE letter from the list {resp}. Check that you return only one letter; if two letters, choose one. No explanations. The answers are:\\n\n",
    "        {question_string}\n",
    "        \"\"\"\n",
    "        prompt_q.append(prompt)\n",
    "        prompt_list.append(question + \" The answers are:\" + question_string)\n",
    "\n",
    "    prompt_llm_list = []\n",
    "    info_res = []\n",
    "    retrieved_results = []\n",
    "\n",
    "    if no_context == 0:\n",
    "        chunk_list = chunks(prompt_list, chunk)\n",
    "        \n",
    "        for chk_el in chunk_list:\n",
    "            tmp = get_results(\n",
    "                query=chk_el,\n",
    "                processor=processor,\n",
    "                model=model,\n",
    "                ds=ds,\n",
    "                all_images=all_images,\n",
    "                top_k=topk,\n",
    "            )\n",
    "            retrieved_results.extend(tmp)\n",
    "\n",
    "        print(f\"Retrieved {len(retrieved_results)} results for context.\")\n",
    "        \n",
    "        for prpmt, ret_files in zip(prompt_q, retrieved_results):\n",
    "            info_el = [el[\"file_name\"].split(\".\")[0] + \"_pg_\" + str(el[\"page_id\"]) for el in ret_files]\n",
    "            info_res.append(info_el)\n",
    "    \n",
    "            context_imgs = [img[\"image\"] for img in ret_files]\n",
    "            conv_imgs = img_context(context_imgs)\n",
    "            prompt_all = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prpmt}] + conv_imgs}]\n",
    "            prompt_llm_list.append(prompt_all)\n",
    "    else:\n",
    "        prompt_llm_list = [[{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prpt}]}] for prpt in prompt_q]\n",
    "        info_res = [\"\"] * len(prompt_q)\n",
    "\n",
    "    # Create a list of tasks to run concurrently\n",
    "    tasks = [get_completion_with_backoff(client, gpt_model, prompt_el, MCQ) for prompt_el in prompt_llm_list]\n",
    "    \n",
    "    # Run tasks concurrently and gather results\n",
    "    completions = await asyncio.gather(*tasks)\n",
    "    \n",
    "    output_array = [comp[\"answer\"] for comp in completions]\n",
    "        \n",
    "    print(f\"Checking list lengths: eval_table {len(table_qa)}, retrieved list {len(retrieved_results)}.\")\n",
    "\n",
    "    return [output_array, info_res]\n",
    "\n",
    "# Wrapper function to run the async function\n",
    "def send_to_model(gpt_model, table_qa, no_context=1, topk=5, chunk=10, processor=[], model=[], ds=[], all_images=[]):\n",
    "    return asyncio.run(send_to_model_async(gpt_model, table_qa, no_context, topk, chunk, processor, model, ds, all_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(MODEL, MODEL_RET, device, qa_data, nr_iter=5, topk=5, chunk=10, out_dir= 'results/evals/', no_context = 0, ds_file=\"data/colpali_pdf_emb.pkl\", pdf_dir='papers_merge/'):\n",
    "    \n",
    "    if MODEL_RET == \"vidore/colpali-v1.3-merged\":\n",
    "        processor = ColPaliProcessor.from_pretrained(MODEL_RET)\n",
    "\n",
    "        model = ColPali.from_pretrained(\n",
    "            MODEL_RET,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=device,\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "        ).eval()\n",
    "    elif MODEL_RET == \"ahmed-masry/ColFlor\":\n",
    "        model = ColFlor.from_pretrained(\n",
    "            MODEL_RET,\n",
    "            device_map=device,\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "        ).eval()\n",
    "\n",
    "        processor = ColFlorProcessor.from_pretrained(MODEL_RET)\n",
    "    \n",
    "    elif MODEL_RET == \"vidore/colSmol-500M\":\n",
    "        model = ColIdefics3.from_pretrained(\n",
    "            MODEL_RET,\n",
    "            device_map=device,\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "        ).eval()\n",
    "\n",
    "        processor = ColIdefics3Processor.from_pretrained(MODEL_RET)\n",
    "\n",
    "    elif MODEL_RET == \"ibm-granite/granite-vision-3.3-2b-embedding\":\n",
    "        model = AutoModel.from_pretrained(\n",
    "            MODEL_RET,\n",
    "            device_map=device,\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "        ).eval()\n",
    "\n",
    "        processor = AutoProcessor.from_pretrained(MODEL_RET)\n",
    "        \n",
    "    elif MODEL_RET == \"vidore/colqwen2.5-v0.2\":\n",
    "        model = ColQwen2_5.from_pretrained(\n",
    "            MODEL_RET,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=device,  # or \"mps\" if on Apple Silicon\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "            ).eval()\n",
    "        processor = ColQwen2_5_Processor.from_pretrained(MODEL_RET)\n",
    "    else:\n",
    "        print(f\"Select correct MODEL_RET, current {MODEL_RET} not correct\")\n",
    "        return -1\n",
    "\n",
    "    \n",
    "\n",
    "    if ds_file != \"\":\n",
    "        with open(ds_file, 'rb') as fp:\n",
    "            ds = pickle.load(fp)\n",
    "    else:\n",
    "        ds = create_document_embeddings(pdf_dir, model, processor, batch_size=4)\n",
    "        with open('data/'+MODEL_RET+'_pdf_emb.pkl', 'wb') as file:\n",
    "           pickle.dump(ds, file)\n",
    "\n",
    "    all_images = convert_pdf_to_images(pdf_dir)\n",
    "\n",
    "    for i in range(nr_iter):\n",
    "        new_data = []\n",
    "        print(f'Processing iteration: {i+1} for model: {MODEL}')\n",
    "\n",
    "        out = send_to_model(MODEL, qa_data, no_context, topk, chunk, processor, model, ds, all_images)\n",
    "        \n",
    "        new_data = qa_data\n",
    "\n",
    "        new_data[\"Model\"]=MODEL\n",
    "        new_data[\"Model_ret\"]=MODEL_RET\n",
    "        new_data[\"Answer\"]=out[0]\n",
    "        new_data[\"Context_papers\"]=out[1]\n",
    "        new_data['Cor_answer'] = 1*(new_data['Answer'] == new_data['Correct'])\n",
    "\n",
    "        new_data.to_csv(out_dir +'eval_' +MODEL_RET.split(\"/\")[-1].split(\"-\")[0] +'_'+ MODEL + '_'+ strftime(\"%Y%m%d%H%M%S\", gmtime()) +'.csv')\n",
    "\n",
    "        print(f'Accuracy: {sum(new_data['Cor_answer'])/len(new_data[\"Answer\"])}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56202d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\"gpt-5\", \"gpt-5-mini\",  \"gpt-5-nano\"]\n",
    "MODEL_RET = [\"vidore/colpali-v1.3-merged\", \"ahmed-masry/ColFlor\", \"vidore/colqwen2.5-v0.2\"] \n",
    "#, \"ibm-granite/granite-vision-3.3-2b-embedding\" \"vidore/colqwen2-v1.0-merged\"\n",
    "TOP_K =5\n",
    "chunk=10\n",
    "nr_iter = 5\n",
    "\n",
    "for model in MODELS:\n",
    "    for model_ret in MODEL_RET:\n",
    "        eval_fn(model, model_ret, \"mps\", qa_data, nr_iter=nr_iter, topk=TOP_K, chunk=chunk, out_dir= 'results/evals/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm_colpali",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
