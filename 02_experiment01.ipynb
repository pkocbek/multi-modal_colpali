{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9054c4a3",
      "metadata": {},
      "source": [
        "# Experiment 01 Evaluation\n",
        "Notebook mirror of `02_experiment01.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd27d427",
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Experiment 01 evaluation pipeline.\n",
        "\n",
        "This script aligns with the revamped 05_experiment02 flow:\n",
        "    * Retrieve both text and image snippets from the configured vector store.\n",
        "    * Build multimodal prompts that embed references inline.\n",
        "    * Query either OpenAI (for GPT models) or a local vLLM endpoint.\n",
        "    * Persist detailed pickle outputs compatible with downstream analysis.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import asyncio\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from pathlib import Path\n",
        "from time import time\n",
        "from typing import Iterable, List\n",
        "\n",
        "import aiohttp\n",
        "import pandas as pd\n",
        "import torch\n",
        "from dotenv import load_dotenv\n",
        "from pydantic import BaseModel, Field\n",
        "from qdrant_client import QdrantClient\n",
        "from transformers import ColPaliForRetrieval, ColPaliProcessor\n",
        "from openai.lib._parsing._completions import type_to_response_format_param\n",
        "\n",
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "from functions import (\n",
        "    retrieve_colpali,\n",
        "    response_real_out,\n",
        "    encode_image_to_data_url,\n",
        "    build_instruction_block,\n",
        "    build_reference_from_metadata,\n",
        "    document_to_context_entry,\n",
        ")\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "BENCHMARK_PATH = Path(\"./data/Glycans_q_a_v5.xlsx\")\n",
        "DEFAULT_TOP_K = 5\n",
        "\n",
        "\n",
        "class MCQ(BaseModel):\n",
        "    \"\"\"Schema enforcing a single-letter response.\"\"\"\n",
        "\n",
        "    answer: str = Field(\n",
        "        description=\"Output is the answer of a MCQ with only one of the following categories: A, B, C or D.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    parser = argparse.ArgumentParser(description=\"Run Experiment 01 evaluation for a single model/RAG mode.\")\n",
        "    parser.add_argument(\"--vllm_port\", type=int, required=True, help=\"Port of the vLLM server (ignored for GPT models).\")\n",
        "    parser.add_argument(\"--model_name\", required=True, help=\"Model identifier.\")\n",
        "    parser.add_argument(\"--filepath_output\", required=True, help=\"Prefix for the pickle output (timestamp appended).\")\n",
        "    parser.add_argument(\"--vector_db\", default=\"\", help=\"Qdrant collection used for retrieval (if applicable).\")\n",
        "    parser.add_argument(\n",
        "        \"--type\",\n",
        "        default=\"\",\n",
        "        choices=[\"\", \"mm_RAG\", \"colpali\"],\n",
        "        help=\"Retrieval type: '' (no RAG), 'mm_RAG', or 'colpali'.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--perm_quest\",\n",
        "        default=\"No\",\n",
        "        help=\"Set to 'Yes' to permute answer order per question.\",\n",
        "    )\n",
        "    parser.add_argument(\"--top_k\", type=int, default=DEFAULT_TOP_K, help=\"Number of retrieved context items.\")\n",
        "    parser.add_argument(\n",
        "        \"--qa_path\",\n",
        "        default=str(BENCHMARK_PATH),\n",
        "        help=\"Path to the Glycans benchmark Excel file.\",\n",
        "    )\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def load_questions(path: Path) -> pd.DataFrame:\n",
        "    \"\"\"Load and shuffle the benchmark rows.\"\"\"\n",
        "    return pd.read_excel(path).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "def should_permute(flag: str) -> bool:\n",
        "    return flag.lower() in {\"yes\", \"true\", \"1\"}\n",
        "\n",
        "\n",
        "class RetrievalManager:\n",
        "    \"\"\"Handles retrieval for the different Experiment 01 RAG setups.\"\"\"\n",
        "\n",
        "    def __init__(self, retrieval_type: str, vector_db: str, top_k: int):\n",
        "        self.retrieval_type = retrieval_type\n",
        "        self.vector_db = vector_db\n",
        "        self.top_k = top_k\n",
        "        self.qdrant_client = None\n",
        "        self.vector_store = None\n",
        "        self.colpali_model = None\n",
        "        self.colpali_processor = None\n",
        "\n",
        "        if retrieval_type == \"mm_RAG\" and vector_db:\n",
        "            self._init_qdrant_store(vector_db)\n",
        "        elif retrieval_type == \"colpali\" and vector_db:\n",
        "            self._init_colpali(vector_db)\n",
        "\n",
        "    def _init_qdrant_store(self, vector_db: str) -> None:\n",
        "        url = os.environ.get(\"QDRANT_URL\", \"http://localhost:6333\")\n",
        "        api_key = os.environ.get(\"QDRANT_API_KEY\")\n",
        "        self.qdrant_client = QdrantClient(url=url, api_key=api_key)\n",
        "        embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\", providers=[\"CUDAExecutionProvider\"])\n",
        "        self.vector_store = QdrantVectorStore(\n",
        "            client=self.qdrant_client,\n",
        "            collection_name=vector_db,\n",
        "            embedding=embeddings,\n",
        "        )\n",
        "\n",
        "    def _init_colpali(self, vector_db: str) -> None:\n",
        "        url = os.environ.get(\"QDRANT_URL\", \"http://localhost:6333\")\n",
        "        api_key = os.environ.get(\"QDRANT_API_KEY\")\n",
        "        self.qdrant_client = QdrantClient(url=url, api_key=api_key)\n",
        "        model_id = \"vidore/colpali-v1.3-hf\"\n",
        "        dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.colpali_model = ColPaliForRetrieval.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=dtype,\n",
        "            device_map=device,\n",
        "        ).eval()\n",
        "        self.colpali_processor = ColPaliProcessor.from_pretrained(model_id)\n",
        "\n",
        "    def fetch(self, query: str) -> list[dict]:\n",
        "        if self.retrieval_type == \"\" or self.top_k <= 0:\n",
        "            return []\n",
        "        if self.retrieval_type == \"mm_RAG\" and self.vector_store is not None:\n",
        "            docs = self.vector_store.similarity_search_with_score(query, self.top_k)\n",
        "            return [document_to_context_entry(doc, score) for doc, score in docs]\n",
        "        if self.retrieval_type == \"colpali\" and self.colpali_model is not None:\n",
        "            result = retrieve_colpali(\n",
        "                query,\n",
        "                self.colpali_processor,\n",
        "                self.colpali_model,\n",
        "                self.qdrant_client,\n",
        "                \"\",\n",
        "                self.vector_db,\n",
        "                self.top_k,\n",
        "            )\n",
        "            entries = []\n",
        "            for point in result.points:\n",
        "                payload = point.payload or {}\n",
        "                metadata = payload.get(\"metadata\", payload)\n",
        "                entries.append(\n",
        "                    {\n",
        "                        \"type\": \"image\",\n",
        "                        \"text\": \"\",\n",
        "                        \"image_path\": metadata.get(\"img_link\"),\n",
        "                        \"reference\": build_reference_from_metadata(metadata),\n",
        "                        \"score\": getattr(point, \"score\", None),\n",
        "                    }\n",
        "                )\n",
        "            return entries\n",
        "        return []\n",
        "\n",
        "\n",
        "def build_messages(question: str, answers: list[str], contexts: list[dict]) -> tuple[list[dict], list[str]]:\n",
        "    \"\"\"Assemble the multimodal prompt and capture reference labels.\"\"\"\n",
        "    instruction = build_instruction_block(question, answers)\n",
        "    content = [{\"type\": \"text\", \"text\": instruction}]\n",
        "    references: list[str] = []\n",
        "\n",
        "    for ctx in contexts:\n",
        "        reference = ctx.get(\"reference\", \"context\")\n",
        "        if ctx.get(\"image_path\") and ctx[\"type\"] == \"image\":\n",
        "            data_url = encode_image_to_data_url(ctx[\"image_path\"])\n",
        "            if data_url:\n",
        "                content.append({\"type\": \"image_url\", \"image_url\": {\"url\": data_url}})\n",
        "                references.append(reference)\n",
        "        if ctx.get(\"text\"):\n",
        "            snippet = ctx[\"text\"].strip()\n",
        "            if snippet:\n",
        "                content.append({\"type\": \"text\", \"text\": f\"[{reference}] {snippet}\"})\n",
        "                if reference not in references:\n",
        "                    references.append(reference)\n",
        "\n",
        "    return [{\"role\": \"user\", \"content\": content}], references\n",
        "\n",
        "\n",
        "async def post_request_with_retries(session, url, headers, data, retries=4, backoff=1):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            async with session.post(url, headers=headers, json=data, timeout=120) as response:\n",
        "                if response.status == 200:\n",
        "                    return await response.json()\n",
        "                text = await response.text()\n",
        "                raise RuntimeError(f\"HTTP {response.status}: {text}\")\n",
        "        except Exception as exc:\n",
        "            if attempt < retries - 1:\n",
        "                await asyncio.sleep(backoff * (2**attempt))\n",
        "            else:\n",
        "                raise exc\n",
        "\n",
        "\n",
        "async def run_inference(\n",
        "    model_name: str,\n",
        "    messages_list: list[list[dict]],\n",
        "    url: str,\n",
        "    headers: dict,\n",
        "    use_schema: bool,\n",
        ") -> list[dict]:\n",
        "    connector = aiohttp.TCPConnector(limit=256)\n",
        "    payloads = []\n",
        "    for messages in messages_list:\n",
        "        body = {\"model\": model_name, \"messages\": messages}\n",
        "        if use_schema:\n",
        "            body[\"response_format\"] = type_to_response_format_param(MCQ)\n",
        "        payloads.append(body)\n",
        "\n",
        "    async with aiohttp.ClientSession(connector=connector) as session:\n",
        "        responses = await asyncio.gather(\n",
        "            *[post_request_with_retries(session, url, headers, data=body) for body in payloads]\n",
        "        )\n",
        "    return responses\n",
        "\n",
        "\n",
        "def prepare_requests(\n",
        "    qa_table: pd.DataFrame,\n",
        "    retrieval: RetrievalManager,\n",
        "    permute_answers: bool,\n",
        ") -> tuple[list[list[dict]], list[dict]]:\n",
        "    messages_list: list[list[dict]] = []\n",
        "    records: list[dict] = []\n",
        "\n",
        "    for _, row in qa_table.iterrows():\n",
        "        answers = [row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"]]\n",
        "        if permute_answers:\n",
        "            perm_idx = random.sample(range(len(answers)), len(answers))\n",
        "        else:\n",
        "            perm_idx = list(range(len(answers)))\n",
        "        shuffled_answers = [answers[i] for i in perm_idx]\n",
        "\n",
        "        contexts = retrieval.fetch(row[\"question\"])\n",
        "        messages, context_refs = build_messages(row[\"question\"], shuffled_answers, contexts)\n",
        "\n",
        "        messages_list.append(messages)\n",
        "        records.append(\n",
        "            {\n",
        "                \"Question_nr\": row[\"Question_nr\"],\n",
        "                \"question\": row[\"question\"],\n",
        "                \"quest_order\": perm_idx,\n",
        "                \"context_refs\": context_refs,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return messages_list, records\n",
        "\n",
        "\n",
        "def extract_answer_text(response_payload: dict) -> str:\n",
        "    choices = response_payload.get(\"choices\", [])\n",
        "    if not choices:\n",
        "        return \"\"\n",
        "    message = choices[0].get(\"message\", {})\n",
        "    content = message.get(\"content\")\n",
        "    if isinstance(content, str):\n",
        "        return content\n",
        "    if isinstance(content, list):\n",
        "        return \"\".join(part.get(\"text\", \"\") for part in content if isinstance(part, dict))\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "async def main() -> None:\n",
        "    args = parse_args()\n",
        "    qa_table = load_questions(Path(args.qa_path))\n",
        "    permute_answers = should_permute(args.perm_quest)\n",
        "    retrieval = RetrievalManager(args.type, args.vector_db, args.top_k)\n",
        "\n",
        "    messages_list, records = prepare_requests(qa_table, retrieval, permute_answers)\n",
        "\n",
        "    if args.model_name.startswith(\"gpt\"):\n",
        "        url = \"https://api.openai.com/v1/chat/completions\"\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        }\n",
        "        use_schema = True\n",
        "    else:\n",
        "        url = f\"http://localhost:{args.vllm_port}/v1/chat/completions\"\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {os.environ.get('VLLM_API_KEY', 'EMPTY')}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        }\n",
        "        use_schema = False\n",
        "\n",
        "    t_start = time()\n",
        "    responses = await run_inference(args.model_name, messages_list, url, headers, use_schema)\n",
        "\n",
        "    out_list = []\n",
        "    for record, raw in zip(records, responses):\n",
        "        parsed_answer = (\n",
        "            raw.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\") if use_schema else extract_answer_text(raw)\n",
        "        )\n",
        "        filt_resp, answer_letter = response_real_out(parsed_answer, record[\"quest_order\"])\n",
        "        out_list.append(\n",
        "            {\n",
        "                **record,\n",
        "                \"answer\": answer_letter,\n",
        "                \"resp_init\": parsed_answer[:50],\n",
        "                \"filt_resp\": filt_resp,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    timestamp = pd.Timestamp(\"now\", tz=\"CET\").strftime(\"%Y%m%d-%H%M%S\")\n",
        "    suffix = \"_perm_q\" if permute_answers else \"\"\n",
        "    eval_results = {\n",
        "        \"model\": args.model_name,\n",
        "        \"evaluation\": sorted(out_list, key=lambda x: x[\"Question_nr\"]),\n",
        "        \"elapsed_time\": time() - t_start,\n",
        "        \"timestamp\": timestamp,\n",
        "        \"permuted_answers\": permute_answers,\n",
        "    }\n",
        "\n",
        "    output_path = Path(f\"{args.filepath_output}_{timestamp}{suffix}.pkl\")\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with output_path.open(\"wb\") as fh:\n",
        "        pickle.dump(eval_results, fh)\n",
        "\n",
        "    print(f\"Saved evaluation results to {output_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mm_rag_gpu_3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
