{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4ba53aca",
      "metadata": {},
      "source": [
        "# Experiment 01 Evaluation Summary\n",
        "Notebook mirror of `04_experiment01_eval.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc107bfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Aggregate Experiment 01 evaluation pickles into accuracy reports.\n",
        "\n",
        "The script scans `results/eval`, reads every pickle produced by\n",
        "`02_experiment01.py`, merges the responses with the benchmark sheet,\n",
        "and exports accuracy summaries (per difficulty, per retrieval setting,\n",
        "and majority-vote tables).\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "BENCHMARK_FILE = \"./data/Glycans_q_a_v5.xlsx\"\n",
        "SUMMARY_PATH = Path(\"results/eval_results.xlsx\")\n",
        "MAJORITY_PATH = Path(\"results/eval_maj_results.xlsx\")\n",
        "FULL_PATH = Path(\"results/eval_full_results.xlsx\")\n",
        "\n",
        "\n",
        "FILE_PATTERN = re.compile(\n",
        "    r\"eval_(?P<model_short>[^_]+)_(?P<vd_name>.+)_(?P<perm_flag>perm|no_perm)_benchmark_(?P<timestamp>\\d{8}-\\d{6})$\"\n",
        ")\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    parser = argparse.ArgumentParser(description=\"Summarise Experiment 01 evaluation pickles.\")\n",
        "    parser.add_argument(\n",
        "        \"--eval-dir\",\n",
        "        default=\"results/evals\",\n",
        "        help=\"Directory containing pickled evaluation files.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--benchmark-path\",\n",
        "        default=BENCHMARK_FILE,\n",
        "        help=\"Path to the benchmark Excel file.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--summary-path\",\n",
        "        default=str(SUMMARY_PATH),\n",
        "        help=\"Output Excel path for per-difficulty accuracy tables.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--majority-path\",\n",
        "        default=str(MAJORITY_PATH),\n",
        "        help=\"Output Excel path for majority-vote accuracy tables.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--full-path\",\n",
        "        default=str(FULL_PATH),\n",
        "        help=\"Output Excel path for the merged raw evaluations.\",\n",
        "    )\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def load_evaluation_pickle(path: Path) -> dict:\n",
        "    import pickle\n",
        "\n",
        "    with path.open(\"rb\") as fh:\n",
        "        return pickle.load(fh)\n",
        "\n",
        "\n",
        "def parse_metadata(path: Path) -> dict:\n",
        "    stem = path.stem\n",
        "    perm_suffix = stem.endswith(\"_perm_q\")\n",
        "    if perm_suffix:\n",
        "        stem = stem[: -len(\"_perm_q\")]\n",
        "\n",
        "    match = FILE_PATTERN.match(stem)\n",
        "    if not match:\n",
        "        raise ValueError(f\"Cannot parse evaluation filename: {path.name}\")\n",
        "\n",
        "    vd_name = match.group(\"vd_name\")\n",
        "    return {\n",
        "        \"model_short\": match.group(\"model_short\"),\n",
        "        \"vd_name\": vd_name,\n",
        "        \"perm_label\": match.group(\"perm_flag\"),\n",
        "        \"timestamp\": match.group(\"timestamp\"),\n",
        "        \"perm_suffix\": perm_suffix,\n",
        "    }\n",
        "\n",
        "\n",
        "def load_table_from_file(path: Path) -> tuple[pd.DataFrame, dict]:\n",
        "    \"\"\"Load evaluation rows and metadata from pickle or CSV formats.\"\"\"\n",
        "    if path.suffix == \".pkl\":\n",
        "        blob = load_evaluation_pickle(path)\n",
        "        df = pd.DataFrame(blob[\"evaluation\"])\n",
        "        meta = {\n",
        "            \"model\": blob.get(\"model\"),\n",
        "            \"elapsed_time\": blob.get(\"elapsed_time\"),\n",
        "            \"timestamp\": blob.get(\"timestamp\"),\n",
        "            \"permuted\": blob.get(\"permuted_answers\"),\n",
        "        }\n",
        "        return df, meta\n",
        "    if path.suffix == \".csv\":\n",
        "        df = pd.read_csv(path)\n",
        "        meta = {\n",
        "            \"model\": df[\"Model\"].iloc[0] if \"Model\" in df.columns else df.get(\"model\", [None])[0],\n",
        "            \"elapsed_time\": df[\"Elapsed\"].iloc[0] if \"Elapsed\" in df.columns else df.get(\"elapsed_time\", [None])[0],\n",
        "            \"timestamp\": df[\"Time_start\"].iloc[0] if \"Time_start\" in df.columns else df.get(\"timestamp\", [None])[0],\n",
        "            \"permuted\": df[\"permuted_answers\"].iloc[0] if \"permuted_answers\" in df.columns else None,\n",
        "        }\n",
        "        return df, meta\n",
        "    raise ValueError(f\"Unsupported evaluation file type: {path}\")\n",
        "\n",
        "\n",
        "def build_dataframe(eval_dir: Path) -> pd.DataFrame:\n",
        "    files = sorted(eval_dir.glob(\"eval_*.pkl\")) + sorted(eval_dir.glob(\"eval_*.csv\"))\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No evaluation outputs (*.pkl / *.csv) found in {eval_dir}\")\n",
        "\n",
        "    frames = []\n",
        "    for result_path in files:\n",
        "        meta = parse_metadata(result_path)\n",
        "        df, payload_meta = load_table_from_file(result_path)\n",
        "        if \"model\" not in df.columns or df[\"model\"].isna().all():\n",
        "            df[\"model\"] = payload_meta.get(\"model\") or meta[\"model_short\"]\n",
        "        df[\"model_short\"] = meta[\"model_short\"]\n",
        "        df[\"vd_name\"] = meta[\"vd_name\"]\n",
        "        df[\"elapsed_time\"] = payload_meta.get(\"elapsed_time\")\n",
        "        df[\"run_timestamp\"] = payload_meta.get(\"timestamp\", meta[\"timestamp\"])\n",
        "        df[\"file_timestamp\"] = meta[\"timestamp\"]\n",
        "        df[\"permuted_answers\"] = payload_meta.get(\"permuted\", meta[\"perm_label\"] == \"perm\")\n",
        "        df[\"filepath\"] = str(result_path)\n",
        "        frames.append(df)\n",
        "\n",
        "    combined = pd.concat(frames, ignore_index=True)\n",
        "    return combined\n",
        "\n",
        "\n",
        "def compute_majority_vote(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    grouped = (\n",
        "        df.groupby(\n",
        "            [\"model_short\", \"model\", \"vd_name\", \"permuted_answers\", \"Question_nr\", \"Difficulty\"],\n",
        "            observed=True,\n",
        "        )[\"Cor_answer\"]\n",
        "        .agg([\"sum\", \"count\"])\n",
        "        .reset_index()\n",
        "    )\n",
        "    grouped[\"Maj_vote\"] = (grouped[\"sum\"] >= np.ceil(grouped[\"count\"] / 2)).astype(int)\n",
        "    pivot = (\n",
        "        grouped.groupby([\"model_short\", \"model\", \"vd_name\", \"permuted_answers\"], observed=True)[\"Maj_vote\"]\n",
        "        .mean()\n",
        "        .reset_index()\n",
        "    )\n",
        "    return pivot\n",
        "\n",
        "\n",
        "def compute_summary_tables(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    pivot = pd.pivot_table(\n",
        "        df,\n",
        "        values=\"Cor_answer\",\n",
        "        index=[\"model_short\", \"model\", \"vd_name\", \"permuted_answers\"],\n",
        "        columns=\"Difficulty\",\n",
        "        aggfunc=\"mean\",\n",
        "        observed=True,\n",
        "    )\n",
        "    pivot = pivot.reindex(columns=[\"Easy\", \"Medium\", \"Hard\"])\n",
        "    return pivot\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    args = parse_args()\n",
        "    eval_dir = Path(args.eval_dir)\n",
        "    benchmark = pd.read_excel(args.benchmark_path)\n",
        "\n",
        "    evaluation_df = build_dataframe(eval_dir)\n",
        "\n",
        "    merged = evaluation_df.merge(\n",
        "        benchmark[[\"Question_nr\", \"Correct\", \"Difficulty\"]],\n",
        "        on=\"Question_nr\",\n",
        "        how=\"left\",\n",
        "    )\n",
        "    merged[\"Cor_answer\"] = (merged[\"answer\"] == merged[\"Correct\"]).astype(int)\n",
        "    merged[\"Difficulty\"] = pd.Categorical(\n",
        "        merged[\"Difficulty\"],\n",
        "        categories=[\"Easy\", \"Medium\", \"Hard\"],\n",
        "        ordered=True,\n",
        "    )\n",
        "    merged[\"vd_name\"] = pd.Categorical(\n",
        "        merged[\"vd_name\"],\n",
        "        categories=[\"no_RAG\", \"text_RAG\", \"mm_RAG\", \"colpali\"],\n",
        "        ordered=True,\n",
        "    )\n",
        "\n",
        "    # Save full merged dataset\n",
        "    full_parent = Path(args.full_path).parent\n",
        "    summary_parent = Path(args.summary_path).parent\n",
        "    majority_parent = Path(args.majority_path).parent\n",
        "    full_parent.mkdir(parents=True, exist_ok=True)\n",
        "    summary_parent.mkdir(parents=True, exist_ok=True)\n",
        "    majority_parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    merged.sort_values(\n",
        "        [\"model_short\", \"vd_name\", \"permuted_answers\", \"Question_nr\"]\n",
        "    ).to_excel(args.full_path, index=False)\n",
        "\n",
        "    # Summary accuracies by difficulty\n",
        "    summary = compute_summary_tables(merged)\n",
        "    with pd.ExcelWriter(args.summary_path) as writer:\n",
        "        summary.to_excel(writer, sheet_name=\"Accuracy\")\n",
        "\n",
        "    # Majority vote statistics\n",
        "    majority = compute_majority_vote(merged)\n",
        "    majority.to_excel(args.majority_path, index=False)\n",
        "\n",
        "    print(f\"[done] Summary saved to {args.summary_path}\")\n",
        "    print(f\"[done] Majority vote saved to {args.majority_path}\")\n",
        "    print(f\"[done] Full evaluations saved to {args.full_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mm_rag_gpu_3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
