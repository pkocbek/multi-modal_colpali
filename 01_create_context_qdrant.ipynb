{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846fd252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.utils.functions import (\n",
    "#     doc_conv, check_ocr, free_memory, format_msgs, pdf_loader,\n",
    "#     modify_orig, delete_papers, get_responses , post_request_with_retries, prompt_prep,\n",
    "#     resize_image, data_preparation, models_local, models_used,get_img_summary, qdrant_process,\n",
    "#     api_models_one_img, save_to_pickle, upsert_to_qdrant,colpali_qdrant,\n",
    "#     encode_image, show_results, convert_pdfs_to_images,\n",
    "#     check_vllm_status, monitor_vllm_process, retrieve_colpali,process_models,\n",
    "#     update_vd_new_user,make_tarfile, extract_tarfile, new_user_set_files\n",
    "#     #CustomRetriever,summarise_context,api_models_imgs\n",
    "\n",
    "# )\n",
    "\n",
    "\n",
    "import docling\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "import torch\n",
    "torch.classes.__path__ = []\n",
    "import onnxruntime as rt\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969875cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264520d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_MODEL = \"google/gemma-3-27b-it\"\n",
    "OPEN_MODEL_SHORT = \"Gemma3-27b\"\n",
    "\n",
    "LI_MODEL = \"vidore/colpali-v1.3-merged\"\n",
    "LI_MODEL_SHORT = \"ColPali\"\n",
    "\n",
    "EMBED_MODEL_ID = \"BAAI/bge-base-en-v1.5\"\n",
    "MAX_TOKENS = 512\n",
    "EMB_DIM = 768\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL_ID,\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")\n",
    "emb_tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_ID )\n",
    "\n",
    "PAPERS_DIR =\"./papers/\"\n",
    "#VD_DIR =\"./src/vectordb/\"\n",
    "QDRANT_PORT= 6333\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, snapshot_download\n",
    "login(token=os.environ[\"HUGGING_FACE_HUB_TOKEN\"])\n",
    "\n",
    "snapshot_download(repo_id=OPEN_MODEL ,cache_dir = os.environ[\"HF_DIR\"] + \"hub/\")#, force_download=True   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    {\"model_name\": \"gpt-4o-2024-11-20\", \"model_short\": \"gpt-4o\", \"port\": \"9999\", \"text_vd\": \"RAG_TEXT\", \"mm_vd\": \"MM_04_GPT_4o\", \"late_inter\": LI_MODEL, \"late_inter_short\": LI_MODEL_SHORT }, \n",
    "    {\"model_name\": \"gpt-4o-mini-2024-07-18\", \"model_short\": \"gpt-4o-mini\", \"port\": \"9999\", \"text_vd\": \"RAG_TEXT\", \"mm_vd\": \"MM_05_GPT_4o_mini\", \"late_inter\": LI_MODEL, \"late_inter_short\": LI_MODEL_SHORT}, \n",
    "    {\"model_name\": OPEN_MODEL, \"model_short\": OPEN_MODEL_SHORT, \"port\": \"8006\", \"text_vd\": \"RAG_TEXT\", \"mm_vd\": \"MM_07_GEMMA3_27B\", \"late_inter\": LI_MODEL, \"late_inter_short\": LI_MODEL_SHORT}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ef926",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [os.path.join(PAPERS_DIR, f) for f in sorted(os.listdir(PAPERS_DIR)) if f.lower().endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f29c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add doi manually!!! or empty list same length as papers\n",
    "# doi_papers = [\n",
    "# \"https://doi.org/10.1038/s41590-024-01916-8\",\n",
    "# \"https://doi.org/10.1186/s12967-018-1695-0\",\n",
    "# \"https://doi.org/10.1097/hjh.0000000000002963\",\n",
    "# ...\n",
    "# ]\n",
    "doi_papers = [\"\" for _ in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65390973",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_multi, processed_text = pdf_loader (papers, doi_papers, [paper.split(\"/\")[-1] for paper in papers],  os.environ[\"VD_DIR\"], emb_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b02125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholder- modify as needed\n",
    "prompt = 'You are an expert in glycan biology and you will be querried. Here is the query: {query}\\nTask:\\nAnswer the clearly and concisely. You will be given Context information, which can be empty.\\n Tone: scientific and concise. It should include critical numeric data, significant results, and relevant keywords if relevant.\\nConstraints:\\nAvoid generic answerd.\\nHere is the Context information: \\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca29455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio \n",
    "\n",
    "dict_outputs = await process_models (processed_multi, prompt, MODELS)\n",
    "dict_outputs[\"text_only\"]= processed_text\n",
    "\n",
    "print(dict_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#colpali and similar model/processor retriever\n",
    "# colpali modules\n",
    "#poetry add git+https://github.com/illuin-tech/colpali.git\n",
    "#from colpali_engine.models import ColIdefics3, ColIdefics3Processor\n",
    "#from colpali_engine.models import ColQwen2_5Omni, ColQwen2_5OmniProcessor\n",
    "#from colpali_engine.models import ColQwen2_5, ColQwen2_5_Processor\n",
    "\n",
    "#poetry add git+https://github.com/AhmedMasryKU/colflor.git\n",
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "from colpali_engine.models import ColFlor, ColFlorProcessor\n",
    "\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from transformers.utils.import_utils import is_flash_attn_2_available\n",
    "\n",
    "def get_ret_model_processor (MODEL_RET, device):\n",
    "    \n",
    "    if MODEL_RET == \"vidore/colpali-v1.3-merged\":\n",
    "        processor = ColPaliProcessor.from_pretrained(MODEL_RET)\n",
    "\n",
    "        model = ColPali.from_pretrained(\n",
    "            MODEL_RET,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=device,\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "        ).eval()\n",
    "    elif MODEL_RET == \"ahmed-masry/ColFlor\":\n",
    "        model = ColFlor.from_pretrained(\n",
    "            MODEL_RET,\n",
    "            device_map=device,\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "        ).eval()\n",
    "\n",
    "        processor = ColFlorProcessor.from_pretrained(MODEL_RET)\n",
    "    \n",
    "    elif MODEL_RET == \"vidore/colSmol-500M\":\n",
    "        model = ColIdefics3.from_pretrained(\n",
    "            MODEL_RET,\n",
    "            device_map=device,\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "        ).eval()\n",
    "\n",
    "        processor = ColIdefics3Processor.from_pretrained(MODEL_RET)\n",
    "\n",
    "    elif MODEL_RET == \"ibm-granite/granite-vision-3.3-2b-embedding\":\n",
    "        model = AutoModel.from_pretrained(\n",
    "            MODEL_RET,\n",
    "            device_map=device,\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "        ).eval()\n",
    "\n",
    "        processor = AutoProcessor.from_pretrained(MODEL_RET)\n",
    "        \n",
    "    elif MODEL_RET == \"vidore/colqwen2.5-v0.2\":\n",
    "        model = ColQwen2_5.from_pretrained(\n",
    "            MODEL_RET,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=device,  # or \"mps\" if on Apple Silicon\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "            ).eval()\n",
    "        processor = ColQwen2_5_Processor.from_pretrained(MODEL_RET)\n",
    "    else:\n",
    "        print(f\"Select correct MODEL_RET, current {MODEL_RET} not correct\")\n",
    "        return -1\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http import models\n",
    "\n",
    "#qdrant should be running in docker !\n",
    "qdrant_client = QdrantClient(url=f\"http://localhost:{QDRANT_PORT}\", api_key=os.environ[\"QDRANT_API_KEY\"])\n",
    "\n",
    "id_text_rag = 0\n",
    "for idx, model in enumerate(MODELS):\n",
    "    if idx == 0 and id_text_rag == 0:\n",
    "        collection_name = model[\"text_vd\"]\n",
    "        if not qdrant_client.collection_exists(collection_name=collection_name):\n",
    "            qdrant_client.create_collection(\n",
    "                collection_name=collection_name,\n",
    "                on_disk_payload=True,\n",
    "                vectors_config=models.VectorParams(size=EMB_DIM, on_disk=True, distance=models.Distance.COSINE),\n",
    "            )\n",
    "        QdrantVectorStore.from_documents(\n",
    "            documents=dict_outputs[model[\"model_short\"]],\n",
    "            url=f\"http://localhost:{QDRANT_PORT}\",\n",
    "            api_key=os.environ[\"QDRANT_API_KEY\"],\n",
    "            collection_name=collection_name,\n",
    "            embedding=embeddings,\n",
    "            )\n",
    "        id_text_rag = 1        \n",
    "\n",
    "    if not qdrant_client.collection_exists(collection_name=model[\"mm_vd\"]):\n",
    "        qdrant_client.create_collection(\n",
    "                collection_name=model[\"mm_vd\"],\n",
    "                on_disk_payload=True,\n",
    "                vectors_config= models.VectorParams(\n",
    "                    size=EMB_DIM,\n",
    "                    on_disk=True, \n",
    "                    distance=models.Distance.COSINE\n",
    "                    ),\n",
    "        )\n",
    "    QdrantVectorStore.from_documents(\n",
    "    documents=dict_outputs[model[\"model_short\"]],\n",
    "    url=f\"http://localhost:{QDRANT_PORT}\",\n",
    "    api_key=os.environ[\"QDRANT_API_KEY\"],\n",
    "    collection_name=model[\"mm_vd\"],\n",
    "    embedding=embeddings,\n",
    "    )\n",
    "\n",
    "    if not qdrant_client.collection_exists(collection_name=model[\"late_inter_short\"]):\n",
    "        qdrant_client.create_collection(\n",
    "            collection_name=model[\"late_inter_short\"],\n",
    "            on_disk_payload=True,\n",
    "            vectors_config=models.VectorParams(\n",
    "                size=vector_size,\n",
    "                distance=models.Distance.COSINE,\n",
    "                on_disk=True,\n",
    "                multivector_config=models.MultiVectorConfig(\n",
    "                    comparator=models.MultiVectorComparator.MAX_SIM\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "    model_li, processor_li = get_ret_model_processor (model[\"late_inter\"], \"cuda\")\n",
    "    points = create_document_embeddings(PAPERS_DIR, model_li, processor_li batch_size=4)\n",
    "    qdrant_client.upsert(collection_name=model[\"late_inter_short\"], points=points)\n",
    "    all_images = convert_pdf_to_images(PAPERS_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
