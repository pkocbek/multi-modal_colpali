{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "846fd252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-11-18 12:20:55.486939268 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card0/device/vendor\"\u001b[m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from functions import (\n",
    "    pdf_loader, process_models\n",
    ")\n",
    "\n",
    "\n",
    "import docling\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "import torch\n",
    "torch.classes.__path__ = []\n",
    "import onnxruntime as rt\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264520d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_MODEL = \"google/gemma-3-27b-it\"\n",
    "OPEN_MODEL_SHORT = \"Gemma3-27b\"\n",
    "\n",
    "LI_MODEL = \"vidore/colpali-v1.3-merged\"\n",
    "LI_MODEL_SHORT = \"ColPali\"\n",
    "\n",
    "EMBED_MODEL_ID = \"BAAI/bge-base-en-v1.5\"\n",
    "MAX_TOKENS = 512\n",
    "EMB_DIM = 768\n",
    "VECTOR_SIZE=128\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL_ID,\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")\n",
    "emb_tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_ID )\n",
    "\n",
    "PAPERS_DIR =\"./papers/\"\n",
    "#VD_DIR =\"./src/vectordb/\"\n",
    "QDRANT_PORT=6333\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb91482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, snapshot_download\n",
    "login(token=os.environ[\"HUGGING_FACE_HUB_TOKEN\"])\n",
    "\n",
    "snapshot_download(repo_id=OPEN_MODEL ,cache_dir = os.environ[\"HF_DIR\"] + \"hub/\")#, force_download=True   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    {\"model_name\": \"gpt-4o-2024-11-20\", \"model_short\": \"gpt-4o\", \"port\": \"9999\", \"text_vd\": \"RAG_TEXT\", \"mm_vd\": \"MM_04_GPT_4o\", \"late_inter\": LI_MODEL, \"late_inter_short\": LI_MODEL_SHORT }, \n",
    "    {\"model_name\": \"gpt-4o-mini-2024-07-18\", \"model_short\": \"gpt-4o-mini\", \"port\": \"9999\", \"text_vd\": \"RAG_TEXT\", \"mm_vd\": \"MM_05_GPT_4o_mini\", \"late_inter\": LI_MODEL, \"late_inter_short\": LI_MODEL_SHORT}, \n",
    "    {\"model_name\": OPEN_MODEL, \"model_short\": OPEN_MODEL_SHORT, \"port\": \"8006\", \"text_vd\": \"RAG_TEXT\", \"mm_vd\": \"MM_07_GEMMA3_27B\", \"late_inter\": LI_MODEL, \"late_inter_short\": LI_MODEL_SHORT}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ef926",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [os.path.join(PAPERS_DIR, f) for f in sorted(os.listdir(PAPERS_DIR)) if f.lower().endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f29c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add doi manually!!! or empty list same length as papers\n",
    "doi_papers = [\n",
    "\"https://doi.org/10.1038/s41590-024-01916-8\",\n",
    "\"https://doi.org/10.1186/s12967-018-1695-0\",\n",
    "\"https://doi.org/10.1097/hjh.0000000000002963\",\n",
    "\"https://doi.org/10.1186/s12967-018-1616-2\",\n",
    "\"https://doi.org/10.3390%2Fbiom13020375\",\n",
    "\"https://doi.org/10.1016/j.bbagen.2017.06.020\",\n",
    "\"https://doi.org/10.1172%2Fjci.insight.89703\",\n",
    "\"https://doi.org/10.1016%2Fj.isci.2022.103897\",\n",
    "\"https://doi.org/10.1002%2Fart.39273\",\n",
    "\"https://doi.org/10.1016/j.bbadis.2018.03.018\",\n",
    "\"https://doi.org/10.1097/MIB.0000000000000372\",\n",
    "\"https://doi.org/10.1053%2Fj.gastro.2018.01.002\",\n",
    "\"https://doi.org/10.1186/s13075-017-1389-7\",\n",
    "\"https://doi.org/10.1021/pr400589m\",\n",
    "\"https://doi.org/10.1161/CIRCRESAHA.117.312174\",\n",
    "\"https://doi.org/10.2337/dc22-0833\",\n",
    "\"https://doi.org/10.1097%2FMD.0000000000003379\",\n",
    "\"https://doi.org/10.1158/1078-0432.CCR-15-1867\",\n",
    "\"https://doi.org/10.1093/gerona/glt190\",\n",
    "\"https://doi.org/10.1111/imr.13407\",\n",
    "\"https://doi.org/10.1053/j.gastro.2018.05.030\",\n",
    "\"https://doi.org/10.1016/j.csbj.2024.03.008\",\n",
    "\"https://doi.org/10.1016/j.cellimm.2018.07.009\",\n",
    "\"https://doi.org/10.1016/j.biotechadv.2023.108169\",\n",
    "\"https://doi.org/10.4049/jimmunol.2400447\"\n",
    "]\n",
    "#doi_papers = [\"\" for _ in papers]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65390973",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_multi, processed_text = pdf_loader (papers, doi_papers, [paper.split(\"/\")[-1] for paper in papers],  os.environ[\"VD_DIR\"], emb_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('prompts_used.pkl', 'rb') as f:\n",
    "    prompt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b02125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #placeholder- modify as needed\n",
    "# prompt = 'You are an expert in glycan biology and you will be querried. Here is the query: {query}\\nTask:\\nAnswer the clearly and concisely. You will be given Context information, which can be empty.\\n Tone: scientific and concise. It should include critical numeric data, significant results, and relevant keywords if relevant.\\nConstraints:\\nAvoid generic answerd.\\nHere is the Context information: \\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca29455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio \n",
    "\n",
    "dict_outputs = await process_models (processed_multi, prompt, MODELS)\n",
    "dict_outputs[\"text_only\"]= processed_text\n",
    "\n",
    "print(dict_outputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f95512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dict_out.pkl', 'wb') as file:\n",
    "    pickle.dump(dict_outputs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict_out.pkl', 'rb') as file:\n",
    "    dict_outputs=pickle.load( file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#colpali and similar model/processor retriever\n",
    "# colpali modules\n",
    "#poetry add git+https://github.com/illuin-tech/colpali.git\n",
    "#from colpali_engine.models import ColIdefics3, ColIdefics3Processor\n",
    "#from colpali_engine.models import ColQwen2_5Omni, ColQwen2_5OmniProcessor\n",
    "#from colpali_engine.models import ColQwen2_5, ColQwen2_5_Processor\n",
    "\n",
    "#poetry add git+https://github.com/AhmedMasryKU/colflor.git\n",
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "#from colpali_engine.models import ColFlor, ColFlorProcessor\n",
    "\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from transformers.utils.import_utils import is_flash_attn_2_available\n",
    "\n",
    "def get_ret_model_processor (MODEL_RET, device):\n",
    "    \n",
    "    if MODEL_RET == \"vidore/colpali-v1.3-merged\":\n",
    "        processor = ColPaliProcessor.from_pretrained(MODEL_RET)\n",
    "\n",
    "        model = ColPali.from_pretrained(\n",
    "            MODEL_RET,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=device,\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "        ).eval()\n",
    "    elif MODEL_RET == \"ahmed-masry/ColFlor\":\n",
    "        model = ColFlor.from_pretrained(\n",
    "            MODEL_RET,\n",
    "            device_map=device,\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "        ).eval()\n",
    "\n",
    "        processor = ColFlorProcessor.from_pretrained(MODEL_RET)\n",
    "    \n",
    "    elif MODEL_RET == \"vidore/colSmol-500M\":\n",
    "        model = ColIdefics3.from_pretrained(\n",
    "            MODEL_RET,\n",
    "            device_map=device,\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "        ).eval()\n",
    "\n",
    "        processor = ColIdefics3Processor.from_pretrained(MODEL_RET)\n",
    "\n",
    "    elif MODEL_RET == \"ibm-granite/granite-vision-3.3-2b-embedding\":\n",
    "        model = AutoModel.from_pretrained(\n",
    "            MODEL_RET,\n",
    "            device_map=device,\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "        ).eval()\n",
    "\n",
    "        processor = AutoProcessor.from_pretrained(MODEL_RET)\n",
    "        \n",
    "    elif MODEL_RET == \"vidore/colqwen2.5-v0.2\":\n",
    "        model = ColQwen2_5.from_pretrained(\n",
    "            MODEL_RET,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=device,  # or \"mps\" if on Apple Silicon\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None,\n",
    "            ).eval()\n",
    "        processor = ColQwen2_5_Processor.from_pretrained(MODEL_RET)\n",
    "    else:\n",
    "        print(f\"Select correct MODEL_RET, current {MODEL_RET} not correct\")\n",
    "        return -1\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "from qdrant_client.http import models\n",
    "\n",
    "def convert_pdf_to_images(pdf_dir):\n",
    "    \"\"\"\n",
    "    Converts all PDFs in a directory to images.\n",
    "\n",
    "    Args:\n",
    "        pdf_dir (str): Path to the directory containing PDFs.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are file names (without extension), \n",
    "              and values are lists of images (one list per PDF).\n",
    "    \"\"\"\n",
    "    pdf_list = [pdf for pdf in sorted(os.listdir(pdf_dir)) if pdf.endswith(\".pdf\")]\n",
    "    all_images = {}\n",
    "\n",
    "    for pdf_file in pdf_list:\n",
    "        pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "        pdf_images = convert_from_path(pdf_path)\n",
    "        all_images[pdf_file] = pdf_images  # Use file name as key\n",
    "    \n",
    "    return all_images\n",
    "\n",
    "def create_document_embeddings(pdf_dir, model, processor, batch_size=2):\n",
    "    \"\"\"\n",
    "    Converts all PDFs in a directory into embeddings with metadata.\n",
    "\n",
    "    Args:\n",
    "        pdf_dir (str): Directory containing PDF files.\n",
    "        model: Pre-trained model for generating embeddings.\n",
    "        processor: Preprocessor for the model (e.g., to process images).\n",
    "        batch_size (int): Batch size for inference.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains:\n",
    "            - \"embedding\": The embedding tensor.\n",
    "            - \"doc_id\": The document ID (int).\n",
    "            - \"page_id\": The page index within the document.\n",
    "            - \"file_name\": The name of the source PDF file.\n",
    "    \"\"\"\n",
    "    all_images = convert_pdf_to_images(pdf_dir)\n",
    "    all_embeddings_with_metadata = []\n",
    "\n",
    "    for doc_id, (file_name, pdf_images) in enumerate(all_images.items()):\n",
    "        dataloader = DataLoader(\n",
    "            dataset=pdf_images,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=lambda x: processor.process_images(x),\n",
    "        )\n",
    "\n",
    "        page_counter = 1\n",
    "        for batch in tqdm(dataloader, desc=f\"Processing {file_name}\"):\n",
    "            with torch.no_grad():\n",
    "                batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "                batch_embeddings = model(**batch)\n",
    "                batch_embeddings = list(torch.unbind(batch_embeddings.to(\"cpu\")))\n",
    "\n",
    "                for embedding in batch_embeddings:\n",
    "                    all_embeddings_with_metadata.append(\n",
    "                        models.PointStruct(\n",
    "                            id=str(uuid.uuid4()),\n",
    "                            vector= embedding,\n",
    "                            payload = {\n",
    "                              \"metadata\": {\n",
    "                                \"doc_id\": doc_id,\n",
    "                                \"page_id\": page_counter,\n",
    "                                \"file_name\": file_name, \n",
    "                                \"img_link\":f\"{os.environ[\"VD_DIR\"]}pg_images/{file_name.split(\".\")[0]}_{page_counter:03d}.png\",\n",
    "                                  # Correctly use the file name\n",
    "                              }    \n",
    "                            }\n",
    "\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                page_counter += 1\n",
    "\n",
    "    return all_embeddings_with_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3643db9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http import models\n",
    "\n",
    "#qdrant should be running in docker !\n",
    "qdrant_client = QdrantClient(url=f\"http://localhost:{QDRANT_PORT}\", api_key=os.environ[\"QDRANT_API_KEY\"])\n",
    "\n",
    "id_text_rag = 0\n",
    "for idx, model in enumerate(MODELS):\n",
    "    if idx == 0 and id_text_rag == 0:\n",
    "        collection_name = model[\"text_vd\"]\n",
    "        if not qdrant_client.collection_exists(collection_name=collection_name):\n",
    "            qdrant_client.create_collection(\n",
    "                collection_name=collection_name,\n",
    "                on_disk_payload=True,\n",
    "                vectors_config=models.VectorParams(size=EMB_DIM, on_disk=True, distance=models.Distance.COSINE),\n",
    "            )\n",
    "        QdrantVectorStore.from_documents(\n",
    "            documents=dict_outputs['text_only'],\n",
    "            url=f\"http://localhost:{QDRANT_PORT}\",\n",
    "            api_key=os.environ[\"QDRANT_API_KEY\"],\n",
    "            collection_name=collection_name,\n",
    "            embedding=embeddings,\n",
    "            )\n",
    "        id_text_rag = 1        \n",
    "\n",
    "    if not qdrant_client.collection_exists(collection_name=model[\"mm_vd\"]):\n",
    "        qdrant_client.create_collection(\n",
    "                collection_name=model[\"mm_vd\"],\n",
    "                on_disk_payload=True,\n",
    "                vectors_config= models.VectorParams(\n",
    "                    size=EMB_DIM,\n",
    "                    on_disk=True, \n",
    "                    distance=models.Distance.COSINE\n",
    "                    ),\n",
    "        )\n",
    "    QdrantVectorStore.from_documents(\n",
    "    documents=dict_outputs[model[\"model_short\"]],\n",
    "    url=f\"http://localhost:{QDRANT_PORT}\",\n",
    "    api_key=os.environ[\"QDRANT_API_KEY\"],\n",
    "    collection_name=model[\"mm_vd\"],\n",
    "    embedding=embeddings,\n",
    "    )\n",
    "\n",
    "    if not qdrant_client.collection_exists(collection_name=model[\"late_inter_short\"]):\n",
    "        qdrant_client.create_collection(\n",
    "            collection_name=model[\"late_inter_short\"],\n",
    "            on_disk_payload=True,\n",
    "            vectors_config=models.VectorParams(\n",
    "                size=VECTOR_SIZE,\n",
    "                distance=models.Distance.COSINE,\n",
    "                on_disk=True,\n",
    "                multivector_config=models.MultiVectorConfig(\n",
    "                    comparator=models.MultiVectorComparator.MAX_SIM\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "    model_li, processor_li = get_ret_model_processor (model[\"late_inter\"], \"cuda\")\n",
    "    points = create_document_embeddings(PAPERS_DIR, model_li, processor_li, batch_size=4)\n",
    "    batch_size = 10\n",
    "    batch_list = [points[n:n+batch_size] for n in range(0, len(points), batch_size)]\n",
    "\n",
    "    for batch_el in batch_list:\n",
    "        qdrant_client.upsert(collection_name=model[\"late_inter_short\"], points=batch_el)\n",
    "    #all_images = convert_pdf_to_images(PAPERS_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm_rag_3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
